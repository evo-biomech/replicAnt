{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO style dataset parser\n",
    "\n",
    "### The generated output includes the following annotation data:\n",
    "* Classes\n",
    "* Bounding Boxes\n",
    "* 2D keypoint data _(poses)_\n",
    "* Polygon masks\n",
    "\n",
    "### Example application(s) (as demonstrated in Plum et al. 2023):\n",
    "* MASK-R-CNN _(semantic and instance segmentation)_\n",
    "\n",
    "### Output structure:\n",
    "* target_dir\n",
    "    * data _(includes all images)_\n",
    "    * labels.json _(includes all annoation data)_\n",
    "    \n",
    "### Notes:\n",
    "\n",
    "* In our example training we have used a modified version of the original [Mask-R-CNN implementation](https://github.com/matterport/Mask_RCNN) which can be found [here](http://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch) to get started on your own semantic segmentation projects. \n",
    "* To view the resulting generated polygon masks, you can use the [COCO_Image_Viewer.ipynb](../evaluation/COCO_Image_Viewer.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pathlib\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required parameters\n",
    "\n",
    "Specify the location of your **generated dataset** and in which **output directory** you wish to save it.\n",
    "\n",
    "**Notes:**\n",
    "* do not include trailing forward slashes in your paths (see examples below)\n",
    "* Your **dataset** name should **NOT include underscores** as they are used to separate passes into their categories. Instead, use hyphens in your naming convention where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location of dataset and return all files\n",
    "dataset_location = \"../example_data/input-multi\"\n",
    "target_dir = \"../example_data/COCO\"\n",
    "SCORER = \"Fabi\"\n",
    "\n",
    "# specify which labels to ignore. By default, all keypoints are written into the dataset\n",
    "# in this example we omit all keypoints relating to wings. Refer to the base_rig documentation for naming conventions\n",
    "omit_labels = ['w_1_l', 'w_1_l_end', 'w_2_l', 'w_2_l_end', 'w_1_r', 'w_1_r_end', 'w_2_r', 'w_2_r_end', 'root']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True to show processing results for each image (disables parallel processing)\n",
    "DEBUG = False\n",
    "\n",
    "# minimise mask complexity by reducing the number of keypoints per mask\n",
    "simple_masks = False\n",
    "\n",
    "# single polygon masks, instead of nested polygons for cutouts\n",
    "single_polygon_mask = False\n",
    "\n",
    "# number of pixels added to pad masks to avoid cutting of contours\n",
    "mask_padding = 5\n",
    "\n",
    "# we can optionally remove occluded points from the dataframe\n",
    "EXCLUDE_OCCLUDED_KEYPOINTS = True\n",
    "\n",
    "enforce_single_class = False # overwrites multiple classes and groups all instances as one\n",
    "\n",
    "# determine the proportion of a bounding box that needs to be filled before considering the visibility as too low\n",
    "visibility_threshold = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines will load the generated dataset from your drive and prepare it for the multi-threaded parsing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f for f in listdir(dataset_location) if isfile(join(dataset_location, f))]\n",
    "all_files.sort()\n",
    "\n",
    "# next, sort files into images, depth maps, segmentation maps, data, and colony info\n",
    "# we only need the location and name of the data files, as all passes follow the same naming convention\n",
    "dataset_data = []\n",
    "dataset_img = []\n",
    "dataset_ID = []\n",
    "dataset_depth = []\n",
    "dataset_norm = []\n",
    "dataset_colony = None\n",
    "\n",
    "for file in all_files:\n",
    "    loc = dataset_location + \"/\" + file\n",
    "    file_info = file.split(\"_\")\n",
    "    \n",
    "    if file_info[1] == \"BatchData\":\n",
    "        dataset_colony = loc\n",
    "        \n",
    "    elif len(file_info) == 2:\n",
    "        # images are available in various formats, but annotation data is always written as json files\n",
    "        if file_info[-1].split(\".\")[-1] == \"json\":\n",
    "            dataset_data.append(loc)\n",
    "        else:\n",
    "            dataset_img.append(loc)\n",
    "            \n",
    "    elif file_info[-1].split(\".\")[0] == \"ID\":\n",
    "        dataset_ID.append(loc)\n",
    "    elif file_info[-1].split(\".\")[0]  == \"depth\":\n",
    "        dataset_depth.append(loc)\n",
    "    elif file_info[-1].split(\".\")[0]  == \"norm\":\n",
    "        dataset_norm.append(loc)\n",
    "        \n",
    "print(\"Found\",len(dataset_data),\"samples...\")\n",
    "\n",
    "# next sort the colony info into its IDs to determine the colony size and individual scales\n",
    "# Opening colony (BatchData) JSON file\n",
    "colony_file = open(dataset_colony)\n",
    " \n",
    "# returns JSON object as a dictionary\n",
    "colony = json.load(colony_file)\n",
    "colony_file.close()\n",
    "\n",
    "\n",
    "\"\"\" !!! requires IDs, model names, scales !!! \"\"\"\n",
    "\n",
    "if not enforce_single_class:\n",
    "    # get provided classes to create a dictionary of class IDs and class names\n",
    "    all_classes = []\n",
    "    for subject in colony[\"Subject Variations\"]:\n",
    "        all_classes.append(colony[\"Subject Variations\"][subject][\"Class\"])\n",
    "        \n",
    "    subject_class_names = []\n",
    "    for class_name in all_classes:\n",
    "        # check if exists in unique_list or not and replace any spaces with underscores\n",
    "        class_name = class_name.replace(\" \", \"_\")\n",
    "        if class_name not in subject_class_names:\n",
    "            # append unique classes \n",
    "            subject_class_names.append(class_name)\n",
    "        \n",
    "    subject_classes = {}\n",
    "    for id,sbj in enumerate(subject_class_names):\n",
    "        subject_classes[str(sbj)] = id\n",
    "else:\n",
    "    subject_class_names = [\"insect_0\"] #np.array([int(0)], dtype=int)\n",
    "    subject_classes = {\"insect\" : 0}\n",
    "\n",
    "print(\"\\nA total of\",len(subject_class_names),\"unique classes have been found.\")\n",
    "print(\"The classes and respective class IDs are:\\n\",subject_classes,\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Loaded colony file with seed\", colony['Seed']) #,\"and\",len(colony['ID']),\"individuals.\")\n",
    "    \n",
    "if len(colony['Subject Variations']) > 1:\n",
    "    multi_animal = True\n",
    "    print(\"Generating MULTI-animal dataset! Containing\",len(colony['Subject Variations']),\"individuals\")\n",
    "else:\n",
    "    multi_animal = False\n",
    "    print(\"Generating SINGLE-animal dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there may be animals for which we don't use all bones we can return a list of all labels and exclude the respective locations from the pose data. As all animals use the same convention, we can simply read in one example and remove the corresponding indices from all animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity we'll assume that at this stage all subjects use the same armature and therefore report the same keypoints\n",
    "# we therefore load the first sample from the list and find the subjects keypoint hierarchy\n",
    "sample_file = open(dataset_data[0])\n",
    "\n",
    "# returns JSON object as a dictionary\n",
    "sample = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "first_entry_key = list(sample[\"iterationData\"][\"subject Data\"][0].keys())[0]\n",
    "labels = list(sample[\"iterationData\"][\"subject Data\"][0][first_entry_key][\"keypoints\"].keys())\n",
    "\n",
    "# show all used labels:\n",
    "print(\"\\nAll labels:  \",labels)\n",
    "\n",
    "print(\"\\nOmitting labels:  \", omit_labels)\n",
    "\n",
    "# removing all occurences of omitted labels from the labels list to be used as keys below\n",
    "labels = [x for x in labels if x not in omit_labels]\n",
    "\n",
    "print(\"\\nUsing labels:  \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a large dictionary to store all our dataset info and then dump it into a single COCO-conform json output file. Refer to [this blogpost](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch) for additional details. Note, that the convention below is specific to the insect base armature and you will need to modify it, in case you use your own custom skeleton conventions. If you are only planning on using this dataset style for segmentation tasks, it exact keypoint and skeleton configuration does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_data = {}\n",
    "\n",
    "from datetime import datetime\n",
    "date = datetime.today().strftime('%d.%m.%Y')\n",
    "\n",
    "# edit any \"info\", \"license\", or \"category\" data here:\n",
    "coco_data[\"info\"] = {\n",
    "        \"description\": \"COCO_Style_FARTS_example_dataset\",\n",
    "        \"url\": \"https://evo-biomech.ic.ac.uk/\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": datetime.today().year,\n",
    "        \"contributor\": \"Fabian Plum, Rene Bulla, David Labonte\",\n",
    "        \"date_created\": date}\n",
    "\n",
    "coco_data[\"licenses\"] = [\n",
    "        {\n",
    "            \"url\": \"http://creativecommons.org/licenses/by/4.0/\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Attribution License\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "coco_data[\"categories\"] = []\n",
    "\n",
    "# we need to explicitly define the following dictionary, to include relationship between keypoints as a skeleton\n",
    "for s,sbj in enumerate(subject_class_names):\n",
    "    coco_data[\"categories\"].append(\n",
    "            {\n",
    "                \"supercategory\": \"insect\",\n",
    "                \"id\": s + 1,\n",
    "                \"name\": sbj,\n",
    "                \"keypoints\":['b_t', 'b_a_1', 'b_a_2', 'b_a_3','b_a_4', 'b_a_5',\n",
    "                             'l_1_co_r', 'l_1_tr_r', 'l_1_fe_r', 'l_1_ti_r', 'l_1_ta_r','l_1_pt_r', \n",
    "                             'l_2_co_r', 'l_2_tr_r', 'l_2_fe_r', 'l_2_ti_r', 'l_2_ta_r', 'l_2_pt_r', \n",
    "                             'l_3_co_r', 'l_3_tr_r', 'l_3_fe_r', 'l_3_ti_r', 'l_3_ta_r', 'l_3_pt_r',\n",
    "                             'l_1_co_l', 'l_1_tr_l', 'l_1_fe_l', 'l_1_ti_l', 'l_1_ta_l', 'l_1_pt_l', \n",
    "                             'l_2_co_l', 'l_2_tr_l', 'l_2_fe_l', 'l_2_ti_l', 'l_2_ta_l', 'l_2_pt_l',\n",
    "                             'l_3_co_l', 'l_3_tr_l', 'l_3_fe_l', 'l_3_ti_l', 'l_3_ta_l', 'l_3_pt_l',\n",
    "                             'b_h', \n",
    "                             'ma_r',\n",
    "                             'an_1_r', 'an_2_r', 'an_3_r',\n",
    "                             'ma_l',\n",
    "                             'an_1_l', 'an_2_l', 'an_3_l'\n",
    "                            ],\n",
    "                \"skeleton\":[\n",
    "                    [1,2],[2,3],[3,4],[4,5],[5,6],\n",
    "                    [1,7],[7,8],[8,9],[9,10],[10,11],[11,12],\n",
    "                    [1,13],[13,14],[14,15],[15,16],[16,17],[17,18],\n",
    "                    [1,19],[19,20],[20,21],[21,22],[22,23],[23,24],\n",
    "                    [1,25],[25,26],[26,27],[27,38],[28,29],[29,30],\n",
    "                    [1,31],[31,32],[32,33],[33,34],[34,35],[35,36],\n",
    "                    [1,37],[37,38],[38,39],[39,40],[40,41],[41,42],\n",
    "                    [1,43],\n",
    "                    [1,44],\n",
    "                    [1,45],[45,46],[46,47],\n",
    "                    [1,48],\n",
    "                    [1,49],[49,50],[50,51]\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    \n",
    "\"\"\"  \n",
    "\n",
    "### COMPLETE SKELETON WHEN INCLUDING WINGS ###\n",
    "\n",
    "for s,sbj in enumerate(subject_class_names):\n",
    "    coco_data[\"categories\"].append(\n",
    "            {\n",
    "                \"supercategory\": \"insect\",\n",
    "                \"id\": s + 1,\n",
    "                \"name\": sbj,\n",
    "                \"keypoints\":['b_t', 'b_a_1', 'b_a_2', 'b_a_3',\n",
    "                             'b_a_4', 'b_a_5', 'b_a_5_end', 'l_1_co_r',\n",
    "                             'l_1_tr_r', 'l_1_fe_r',  'l_1_ti_r', 'l_1_ta_r', \n",
    "                             'l_1_pt_r', 'l_1_pt_r_end', 'l_2_co_r', 'l_2_tr_r', \n",
    "                             'l_2_fe_r', 'l_2_ti_r', 'l_2_ta_r', 'l_2_pt_r', \n",
    "                             'l_2_pt_r_end', 'l_3_co_r', 'l_3_tr_r', 'l_3_fe_r', \n",
    "                             'l_3_ti_r', 'l_3_ta_r', 'l_3_pt_r', 'l_3_pt_r_end',\n",
    "                             'w_1_r', 'w_1_r_end',  'w_2_r', 'w_2_r_end',\n",
    "                             'l_1_co_l', 'l_1_tr_l', 'l_1_fe_l', 'l_1_ti_l',\n",
    "                             'l_1_ta_l', 'l_1_pt_l', 'l_1_pt_l_end', 'l_2_co_l', \n",
    "                             'l_2_tr_l', 'l_2_fe_l', 'l_2_ti_l', 'l_2_ta_l',\n",
    "                             'l_2_pt_l', 'l_2_pt_l_end', 'l_3_co_l', 'l_3_tr_l',\n",
    "                             'l_3_fe_l', 'l_3_ti_l', 'l_3_ta_l', 'l_3_pt_l',\n",
    "                             'l_3_pt_l_end', 'w_1_l', 'w_1_l_end', 'w_2_l',\n",
    "                             'w_2_l_end', 'b_h', 'ma_r', 'ma_r_end',\n",
    "                             'an_1_r', 'an_2_r', 'an_3_r', 'an_3_r_end',\n",
    "                             'ma_l', 'ma_l_end', 'an_1_l', 'an_2_l', \n",
    "                             'an_3_l', 'an_3_l_end'],\n",
    "                \"skeleton\":[\n",
    "                    [2,1],[3,2],[4,3],\n",
    "                    [5,4],[6,5],[7,6],\n",
    "                    [9,8],[10,9],[11,10],[12,11],\n",
    "                    [13,12],[14,13],[16,15],\n",
    "                    [17,16],[18,17],[19,18],[20,19],\n",
    "                    [21,20],[23,22],[24,23],\n",
    "                    [25,24],[26,25],[27,26],[28,27],\n",
    "                    [30,29],[32,31],\n",
    "                    [34,33],[35,34],[36,35],\n",
    "                    [37,36],[38,37],[39,38],\n",
    "                    [41,40],[42,41],[43,42],[44,43],\n",
    "                    [45,44],[46,45],[48,47],\n",
    "                    [49,48],[50,49],[51,50],[52,51],\n",
    "                    [53,52],[55,54],\n",
    "                    [57,56],[58,1],[59,58],[60,59],\n",
    "                    [61,58],[62,61],[63,62],[64,63],\n",
    "                    [65,58],[66,65],[67,58],[68,67],\n",
    "                    [69,68],[70,69]\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "# when adding images in the next step the following info needs to be given:\n",
    "coco_data[\"images\"] = []\n",
    "\n",
    "# FORMATTING NOTES [\"images\"]\n",
    "\n",
    "\"\"\"\n",
    "\"images\": [\n",
    "    {\n",
    "        \"id\": ###### (-> generated ID, use i, needs to the same for annoations),\n",
    "        \"license\": 1,\n",
    "        \"width\": display_img.shape[0],\n",
    "        \"height\": display_img.shape[0],\n",
    "        \"file_name\": img.split('/')[-1][:-4] + \"_synth\" + \".JPG\"\n",
    "    },\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "# FORMATTING NOTES [\"annotations\"]\n",
    "\n",
    "\"\"\"\n",
    "\"annotations\": [\n",
    "    {\n",
    "        \"segmentation\": [[x0,y0,x1,y1...xn,yn][x_0,y_0,...x_n,y_n]] (-> coordinates of mask outline, if seperated, multiple arrays can be passed),\n",
    "        \"area\": #### (-> = to the sum of pixels inside the mask),\n",
    "        \"iscrowd\": 0 (as we treat all individuals seperately),\n",
    "        \"image_id\": # (-> = i when iterating over all images),\n",
    "        \"bbox\": [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]] (-> unlike darknet the original (sub-)pixel values are used here),\n",
    "        \"category_id\": 1 (-> for now there is only one category, replace with class ID for multi class),\n",
    "        \"id\": ##### (-> separate counter to i and im)\n",
    "    },\n",
    "\"\"\"\n",
    "\n",
    "# each individual in the dataset is treated as a sparate annotation with a corresponding image ID\n",
    "coco_data[\"annotations\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all dataset related parameters configured, we have provided a multi-threaded parsing solution below to minimise the processing time it takes to bring the entire dataset into the required output format. Currently, we instanciate one processing thread per (virtual) CPU core but you can adjust this value if you wish by changing:\n",
    "\n",
    "```\n",
    "threadList_export = createThreadList(#NumDesiredThreads)\n",
    "```\n",
    "\n",
    "**Note:** To see the process of mask generation from ID passes in action, set the **DEBUG** mode to **\"True\"**. This will however slow down the processing speed considerably and only run in single-threaded mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThreads():\n",
    "    \"\"\" Returns the number of available threads on a posix/win based system \"\"\"\n",
    "    if sys.platform == 'win32':\n",
    "        return int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "    else:\n",
    "        return int(os.popen('grep -c cores /proc/cpuinfo').read())\n",
    "\n",
    "class exportThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, q):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.q = q\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting \" + self.name)\n",
    "        process_detections(self.name, self.q)\n",
    "        print(\"Exiting \" + self.name)\n",
    "        \n",
    "def createThreadList(num_threads):\n",
    "    threadNames = []\n",
    "    for t in range(num_threads):\n",
    "        threadNames.append(\"Thread_\" + str(t))\n",
    "\n",
    "    return threadNames\n",
    "\n",
    "def process_detections(threadName, q):\n",
    "    while not exitFlag_export:\n",
    "        queueLock.acquire()\n",
    "        if not workQueue_export.empty():\n",
    "            \n",
    "            data_input = q.get()\n",
    "            i, data_loc, img, ID = data_input\n",
    "            queueLock.release()\n",
    "            \n",
    "            display_img = cv2.imread(img)\n",
    "            display_img_orig = display_img.copy()\n",
    "            \n",
    "            # compute visibility for each individual\n",
    "            seg_img = cv2.imread(ID)\n",
    "            seg_img_display = seg_img.copy()\n",
    "            \n",
    "            data_file = open(data_loc)\n",
    "            # returns JSON object as a dictionary\n",
    "            data = json.load(data_file)\n",
    "            data_file.close()\n",
    "            \n",
    "            img_shape = display_img.shape\n",
    "            \n",
    "            # only add images that contain visibile individuals\n",
    "            is_empty = True\n",
    "            \n",
    "            img_name = target_dir + \"/data/\" + img.split('/')[-1][:-4] + \"_synth\" + \".jpg\"\n",
    "\n",
    "            img_info = []\n",
    "            \n",
    "            # check if the size of the image and segmentation pass match\n",
    "            if display_img.shape != seg_img.shape:\n",
    "                print(\"Size mismatch of image and segmentation pass for sample\",data_input[1].split(\"/\")[-1],\"!\")\n",
    "            else:\n",
    "                for im, individual in enumerate(data[\"iterationData\"][\"subject Data\"]):\n",
    "                    ind_key = list(individual.keys())[0]\n",
    "                    ind_ID = int(ind_key)\n",
    "                    # WARNING ID numbering begins at 1\n",
    "\n",
    "                    fontColor = (int(ID_colours[ind_ID,0]),\n",
    "                                 int(ID_colours[ind_ID,1]),\n",
    "                                 int(ID_colours[ind_ID,2]))\n",
    "                    \n",
    "                    bbox_orig = [individual[ind_key][\"2DBounds\"][\"xmin\"],\n",
    "                                 individual[ind_key][\"2DBounds\"][\"ymin\"],\n",
    "                                 individual[ind_key][\"2DBounds\"][\"xmax\"],\n",
    "                                 individual[ind_key][\"2DBounds\"][\"ymax\"]]\n",
    "                    \n",
    "                    bbox = fix_bounding_boxes(bbox_orig, max_val=display_img.shape)\n",
    "                    \n",
    "                    # only process an individual if its bounding box width and height are not zero\n",
    "                    if bbox[2] - bbox[0] == 0 or bbox[3] - bbox[1] == 0:\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    contours_lowpoly = []\n",
    "\n",
    "                    #try:\n",
    "                    ID_mask = cv2.inRange(seg_img[bbox[1]:bbox[3],bbox[0]:bbox[2]], np.array([0, 0, ind_ID - 0]), np.array([0, 0, ind_ID + 0]))\n",
    "                    indivual_occupancy = cv2.countNonZero(ID_mask)\n",
    "\n",
    "                    # the kernel size for both dilation and median blur are to be determined by the bbounding boxes relative size\n",
    "                    rel_size = ((bbox[2] - bbox[0]) / display_img.shape[0] + (bbox[3] - bbox[1]) / display_img.shape[0]) / 2\n",
    "                    # values range from 0 (tiny) to 1 (huge)\n",
    "                    # required smoothing 5 to 95\n",
    "                    rel_size_root = int(round((15 * rel_size)/2.)*2 + 1) # round to next odd integer\n",
    "                    #print(\"img:\", i, \"individual:\", im, \"rel_size\", rel_size, rel_size_root)\n",
    "\n",
    "                    # to simplify the generated masks and counter compression artifacts the original mask is dilated\n",
    "                    # https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html\n",
    "                    kernel = np.ones((rel_size_root, rel_size_root), 'uint8')\n",
    "                    ID_mask_dilated = cv2.dilate(ID_mask, kernel, iterations=1)\n",
    "                    # use median blur to further smooth the edges of the binary mask\n",
    "                    ID_mask_dilated = cv2.medianBlur(ID_mask_dilated,rel_size_root)\n",
    "\n",
    "                    # pad segmentation subwindow to prevent contours from being cut off\n",
    "\n",
    "                    if mask_padding != 0:\n",
    "                        ID_mask_dilated_padded = np.zeros([ID_mask_dilated.shape[0] + mask_padding * 2 , ID_mask_dilated.shape[1] + mask_padding * 2], 'uint8')\n",
    "                        ID_mask_dilated_padded[mask_padding:-mask_padding,mask_padding:-mask_padding] = ID_mask_dilated\n",
    "                        ID_mask_dilated = ID_mask_dilated_padded\n",
    "\n",
    "                    # find contours using cv2.CHAIN_APPROX_SIMPLE to minimise the number of control points\n",
    "                    # use cv2.RETR_EXTERNAL instead of cv2.RETR_TREE to only return the outer most contours\n",
    "                    # depending on the version of openCV the function findContours additionally returns the image\n",
    "                    if simple_masks:\n",
    "                        chain_approx = cv2.CHAIN_APPROX_SIMPLE\n",
    "                    else:\n",
    "                        chain_approx = cv2.CHAIN_APPROX_TC89_KCOS\n",
    "\n",
    "                    if single_polygon_mask:\n",
    "                        polygon_method = cv2.RETR_TREE\n",
    "                    else:\n",
    "                        polygon_method = cv2.RETR_EXTERNAL\n",
    "\n",
    "                    try:\n",
    "                        contours, hierarchy = cv2.findContours(ID_mask_dilated, polygon_method, chain_approx)\n",
    "                    except:\n",
    "                        useless_img, contours, hierarchy = cv2.findContours(ID_mask_dilated, polygon_method, chain_approx)\n",
    "                    # now sort contours by area and only keep the 4 largest parts \n",
    "                    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "                    if len(contours) > 3:\n",
    "                        contours = contours[:4]\n",
    "\n",
    "                    # finally we simplify the generated contours to decrease memory usage\n",
    "                    # and fascilitate correct processing, using polygon approximation\n",
    "                    for contour in contours:\n",
    "                        # decrease epsilon for finer contours\n",
    "                        contours_lowpoly.append(cv2.approxPolyDP(contour, epsilon=1, closed=True))\n",
    "\n",
    "\n",
    "                    if DEBUG:\n",
    "                        print(\"\\nindividual\",im,ID_mask_dilated.dtype)\n",
    "                        print(hierarchy)\n",
    "                        # draw the contours on the empty image\n",
    "                        seg_img_display = seg_img.copy()\n",
    "                        cv2.imshow(\"mask: \", ID_mask_dilated)\n",
    "                        cv2.drawContours(seg_img_display[bbox[1]:bbox[3],bbox[0]:bbox[2]], contours, -1, (255,0,0), 3)\n",
    "                        cv2.imshow(\"segmentation: \", seg_img_display[bbox[1]:bbox[3],bbox[0]:bbox[2]])\n",
    "                        cv2.waitKey(0)\n",
    "\n",
    "                    #except:\n",
    "                    #    indivual_occupancy = 1\n",
    "\n",
    "                    #indivual_occupancy = np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 - 1)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 + 1)]).all(axis = 2))\n",
    "                    bbox_area = abs((bbox[2] - bbox[0]) * (bbox[3] - bbox[1])) + 1\n",
    "                    bbox_occupancy = indivual_occupancy / bbox_area\n",
    "                    #print(\"Individual\", individual[0], \"with bounding box occupancy \",bbox_occupancy)\n",
    "\n",
    "                    if not enforce_single_class:\n",
    "                        class_ID = subject_classes[colony['Subject Variations'][ind_key][\"Class\"].replace(\" \",\"_\")]\n",
    "                    else:\n",
    "                        # here we use a single class, otherwise this can be replaced by size / scale values\n",
    "                        class_ID = 0\n",
    "                        \n",
    "                    #cv2.putText(display_img, \"ID: \" + str(int(individual[0])), (bbox[0] + 10,bbox[3] - 10), font, fontScale, fontColor, lineType)\n",
    "                    if bbox_occupancy > visibility_threshold:\n",
    "                        #cv2.rectangle(display_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), fontColor, 2)\n",
    "\n",
    "                        # collect all joint info and convert into COCO readable format\n",
    "                        # \"keypoints\" are arrays of length 3K, K is the total number of key points defined for a class \n",
    "                        # [x, y, v] with the key point visibility v:\n",
    "\n",
    "                        # v=0   Indicates that this key point is not marked (in this case x=y=v=0ï¼‰\n",
    "                        # v=1   Indicates that this key point is marked but not visible(Obscured)\n",
    "                        # v=2   Indicates that this key point is marked and visible at the same time\n",
    "\n",
    "                        # let's binarise the image and dilate it to make sure all visible keypoints are found\n",
    "                        if not multi_animal:\n",
    "                            seg_bin = cv2.inRange(seg_img, np.array([0,0, 1]), np.array([0,0, 3]))\n",
    "                        else:\n",
    "                            seg_bin = cv2.inRange(seg_img, np.array([0,0, ind_ID - 2]), np.array([0,0, ind_ID + 2]))\n",
    "                        kernel = np.ones((5,5), np.uint8)\n",
    "                        seg_bin_dilated = cv2.dilate(seg_bin,kernel,iterations = 2)\n",
    "\n",
    "                        keypoints = []\n",
    "                        img_shape = display_img.shape\n",
    "                        for point in range(len(labels)):\n",
    "                            # check if point is located within the image\n",
    "                            if individual[ind_key][\"keypoints\"][labels[point]][\"2DPos\"][\"x\"] > img_shape[0] or individual[ind_key][\"keypoints\"][labels[point]][\"2DPos\"][\"x\"] < 0 or individual[ind_key][\"keypoints\"][labels[point]][\"2DPos\"][\"y\"] > img_shape[1] or individual[ind_key][\"keypoints\"][labels[point]][\"2DPos\"][\"y\"] < 0:\n",
    "                                keypoints.extend([0,0,0]) # x=y=v=0 -> ignore keypoint\n",
    "                            else:\n",
    "                                # if it is, check its visibility\n",
    "                                x_temp = int(individual[ind_key][\"keypoints\"][labels[point]][\"2DPos\"][\"x\"])\n",
    "                                y_temp = int(individual[ind_key][\"keypoints\"][labels[point]][\"2DPos\"][\"y\"])  \n",
    "                                if seg_bin_dilated[y_temp,x_temp] == 255:                   \n",
    "                                    visibility_pt = 2 # point is visible\n",
    "                                else:\n",
    "                                    visibility_pt = 1 # point is marked but obstructed\n",
    "\n",
    "                                keypoints.extend([x_temp,y_temp,visibility_pt])\n",
    "                            # cv2.circle(display_img, (int(individual[point*2 + 5]),int(individual[point*2 + 6])), radius=3, color=fontColor, thickness=-1)\n",
    "                            # let's see of this is really the centre\n",
    "\n",
    "\n",
    "\n",
    "                        if generate_dataset:\n",
    "                            # now we need to convert all the info into the desired format.\n",
    "                            segmentation_mask= []\n",
    "\n",
    "                            new_bbox = [display_img.shape[0],display_img.shape[1],0,0]\n",
    "                            mask_area = 0\n",
    "\n",
    "                            if len(contours_lowpoly) != 0:\n",
    "                                for contour in contours_lowpoly:\n",
    "                                    mask_area += cv2.contourArea(contour)\n",
    "                                    sub_mask = []\n",
    "                                    for coords in contour:\n",
    "                                        sub_mask_x = int(bbox[0] + coords[0,0])\n",
    "                                        sub_mask_y = int(bbox[1] + coords[0,1])\n",
    "                                        sub_mask.append(sub_mask_x)\n",
    "                                        sub_mask.append(sub_mask_y)\n",
    "\n",
    "                                        if sub_mask_x < new_bbox[0]:\n",
    "                                            new_bbox[0] = sub_mask_x\n",
    "                                        if sub_mask_x > new_bbox[2]:\n",
    "                                            new_bbox[2] = sub_mask_x\n",
    "\n",
    "                                        if sub_mask_y < new_bbox[1]:\n",
    "                                            new_bbox[1] = sub_mask_y\n",
    "                                        if sub_mask_y > new_bbox[3]:\n",
    "                                            new_bbox[3] = sub_mask_y\n",
    "\n",
    "                                    if len(sub_mask) >= 8:\n",
    "                                        # only include polygons with at least 4 vertices\n",
    "                                        segmentation_mask.append(sub_mask)\n",
    "                                        is_empty = False\n",
    "\n",
    "                            # now that we have a clean segmentation mask, we can refine the bounding box as well\n",
    "\n",
    "                            if not is_empty:\n",
    "                                coco_data[\"annotations\"].append({\n",
    "                                        \"segmentation\": segmentation_mask, # (-> coordinates of mask outline, if seperated, multiple arrays can be passed),\n",
    "                                        \"area\": mask_area, # (-> = to the sum of pixels inside the mask),\n",
    "                                        \"iscrowd\": 0, #(as we treat all individuals seperately),\n",
    "                                        \"image_id\":  i, # (-> = i when iterating over all images),\n",
    "                                        \"bbox\": [int(new_bbox[0]), int(new_bbox[1]), int(new_bbox[2]-new_bbox[0]), int(new_bbox[3]-new_bbox[1])], # (-> unlike darknet the original (sub-)pixel values are used here),\n",
    "                                        \"category_id\": class_ID + 1, # (-> classes start at 1 in COCO, there is only one \"supercategory\" for now),\n",
    "                                        \"id\": int(str(i) + \"000\" + str(ind_ID)), # (-> joining i and ind_ID)\n",
    "                                        \"keypoints\": keypoints\n",
    "                                        })\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                        # create mask to highlight low visibility animals\n",
    "                        #blk = np.zeros(display_img.shape, np.uint8)\n",
    "                        #cv2.rectangle(blk, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "                        # display original bounding box\n",
    "                        #cv2.rectangle(display_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), fontColor, 2)\n",
    "                        # add text to discarded ID\n",
    "                        #cv2.putText(display_img, \"OCCLUDED\", (bbox[0] + 10,bbox[3] - 35), font, fontScale, (0,0,255), lineType)\n",
    "\n",
    "                        # blend the mask with original image\n",
    "                        #display_img = cv2.addWeighted(display_img, 1.0, blk, 0.25, 1)\n",
    "\n",
    "                        #print(\"Individual\", int(individual[0]), \"has been discarded due to excessive occlusion.\")\n",
    "                        #print(\"expected:\",int((individual[0]/len(colony['ID']))*255))\n",
    "\n",
    "\n",
    "\n",
    "            # uncomment to show resulting bounding boxes and masks\n",
    "            if len(threadList_export) == 1:\n",
    "                cv2.imshow(\"segmentation: \" ,cv2.resize(seg_img_display, (int(seg_img.shape[1] / 2), \n",
    "                                                                  int(seg_img.shape[0] / 2))))\n",
    "                cv2.imshow(\"labeled image\", cv2.resize(display_img, (int(display_img.shape[1] / 2), \n",
    "                                                                     int(display_img.shape[0] / 2))))\n",
    "                cv2.waitKey(1)\n",
    "            \n",
    "            \n",
    "            if not is_empty:\n",
    "                coco_data[\"images\"].append({\n",
    "                        \"id\": i,\n",
    "                        \"license\": 1,\n",
    "                        \"width\": int(display_img.shape[0]),\n",
    "                        \"height\": int(display_img.shape[1]),\n",
    "                        \"file_name\": img.split('/')[-1][:-4] + \"_synth\" + \".JPG\"\n",
    "                    }\n",
    "                )\n",
    "                cv2.imwrite(img_name, display_img)\n",
    "                print(\"Saved\", img_name)\n",
    "            \n",
    "        else:\n",
    "            queueLock.release()\n",
    "            \n",
    "# setup as many threads as there are (virtual) CPUs\n",
    "exitFlag_export = 0\n",
    "if DEBUG:\n",
    "    threadList_export = createThreadList(1)\n",
    "else:\n",
    "    threadList_export = createThreadList(getThreads())\n",
    "print(\"Using\", len(threadList_export), \"threads for export...\")\n",
    "queueLock = threading.Lock()\n",
    "\n",
    "# define paths to all images and set the maximum number of items in the queue equivalent to the number of images\n",
    "workQueue_export = queue.Queue(len(dataset_img))\n",
    "threads = []\n",
    "threadID = 1\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "ID_colours = np.random.randint(255, size=(255, 3))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.5\n",
    "lineType = 2\n",
    "\n",
    "# we can additionally plot the points in the data files to check joint locations\n",
    "plot_joints = True\n",
    "\n",
    "# remember to refine an export folder when saving out your dataset\n",
    "generate_dataset = True\n",
    "\n",
    "def fix_bounding_boxes(coords,max_val = [1024,1024]):\n",
    "    # fix bounding box coordinates so they do not reach beyond the image\n",
    "    fixed_coords = []\n",
    "    for c, coord in enumerate(coords):\n",
    "        if c == 0 or c == 2:\n",
    "            max_val_temp = max_val[0]\n",
    "        else:\n",
    "            max_val_temp = max_val[1]\n",
    "            \n",
    "        if coord >= max_val_temp:\n",
    "            coord = max_val_temp\n",
    "        elif coord <= 0:\n",
    "            coord = 0\n",
    "        \n",
    "        fixed_coords.append(int(coord))\n",
    "        \n",
    "    return fixed_coords\n",
    "\n",
    "timer = time.time()\n",
    "\n",
    "# create output folder for used images\n",
    "if not os.path.exists(target_dir + \"/data\"):\n",
    "    os.mkdir(target_dir + \"/data\")\n",
    "\n",
    "# Create new threads\n",
    "for tName in threadList_export:\n",
    "    thread = exportThread(threadID, tName, workQueue_export)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "\n",
    "# Fill the queue with samples\n",
    "queueLock.acquire()\n",
    "for i, (data, img, ID) in enumerate(zip(dataset_data , dataset_img, dataset_ID)):\n",
    "    workQueue_export.put([i, data, img, ID])\n",
    "queueLock.release()\n",
    "\n",
    "# Wait for queue to empty\n",
    "while not workQueue_export.empty():\n",
    "    pass\n",
    "\n",
    "# Notify threads it's time to exit\n",
    "exitFlag_export = 1\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(\"Exiting Main export Thread\")\n",
    "\n",
    "# close all windows if they were opened\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Total time elapsed:\",time.time()-timer,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dump it all into one **COCO style json** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_dir + '/labels.json', 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(coco_data, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
